Curent state after creation of LXC:
root@DyvoPes ~$ ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether 00:25:90:e3:3b:34 brd ff:ff:ff:ff:ff:ff
    altname enp1s0f0
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:25:90:e3:3b:35 brd ff:ff:ff:ff:ff:ff
    altname enp1s0f1
    inet 66.85.133.174/26 brd 66.85.133.191 scope global eth1
       valid_lft forever preferred_lft forever
    inet 66.85.133.175/26 brd 66.85.133.191 scope global secondary eth1
       valid_lft forever preferred_lft forever
    inet6 fe80::225:90ff:fee3:3b35/64 scope link 
       valid_lft forever preferred_lft forever
7: lxdbr0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 00:16:3e:09:53:79 brd ff:ff:ff:ff:ff:ff
    inet 10.249.49.1/24 scope global lxdbr0
       valid_lft forever preferred_lft forever
    inet6 fd42:7b3e:a6d5:fce::1/64 scope global 
       valid_lft forever preferred_lft forever
    inet6 fe80::216:3eff:fe09:5379/64 scope link 
       valid_lft forever preferred_lft forever
9: vetha87afdf7@if8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master lxdbr0 state UP group default qlen 1000
    link/ether ca:e9:66:c3:c8:f6 brd ff:ff:ff:ff:ff:ff link-netnsid 0
13: vethb5f7b8af@if12: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master lxdbr0 state UP group default qlen 1000
    link/ether 9e:1c:4f:1e:1c:e9 brd ff:ff:ff:ff:ff:ff link-netnsid 2
    
    
root@DyvoPes ~$ lxc list
+-----------------+---------+----------------------+----------------------------------------------+-----------+-----------+----------+
|      NAME       |  STATE  |         IPV4         |                     IPV6                     |   TYPE    | SNAPSHOTS | LOCATION |
+-----------------+---------+----------------------+----------------------------------------------+-----------+-----------+----------+
| test-container  | RUNNING | 10.249.49.103 (eth0) | fd42:7b3e:a6d5:fce:216:3eff:fe34:25f7 (eth0) | CONTAINER | 0         | DyvoPes  |
+-----------------+---------+----------------------+----------------------------------------------+-----------+-----------+----------+
| wp-db-container | RUNNING | 10.249.49.222 (eth0) | fd42:7b3e:a6d5:fce:216:3eff:feef:d4da (eth0) | CONTAINER | 0         | DyvoPes  |
+-----------------+---------+----------------------+----------------------------------------------+-----------+-----------+----------+



________________________________________________________    

Changes to assign a public IP to containers: 
_______________________________________________________ 

root@DyvoPes ~$ nm-connection editor > GUI will appear click '+' sign > select Bringe in Virtual section > Bridge0 > Choose a connection type: Ethernet >  eth1 (my device with the Internet ) > Save > Save and then a new bridge will appear in the list in GUI.

root@DyvoPes ~$ nmcli connection up "Bridge connection 1"
Successfully activated bridge connection

Reboot the server. 
After that eth1 will not longer be the Internet interface though it will be up. Brindge device will contain the public IP instead. 
Next steps is to assign it to containers. 

root@DyvoPes ~$ lxc  stop test-container 
root@DyvoPes ~$ lxc config device add test-container  eth0 nic nictype=bridged parent=bridge0 name=eth0
root@DyvoPes ~$ Device eth0 added to  test-container
root@DyvoPes ~$ lxc  start test-containe
root@DyvoPes ~$ lxc list
+-----------------+---------+----------------------+----------------------------------------------+-----------+-----------+----------+
|      NAME       |  STATE  |         IPV4         |                     IPV6                     |   TYPE    | SNAPSHOTS | LOCATION |
+-----------------+---------+----------------------+----------------------------------------------+-----------+-----------+----------+
| test-container  | RUNNING | 66.85.133.175 (eth0) | fd42:7b3e:a6d5:fce:216:3eff:fe34:25f7 (eth0) | CONTAINER | 0         | DyvoPes  |
+-----------------+---------+----------------------+----------------------------------------------+-----------+-----------+----------+
| wp-db-container | RUNNING | 10.249.49.222 (eth0) | fd42:7b3e:a6d5:fce:216:3eff:feef:d4da (eth0) | CONTAINER | 0         | DyvoPes  |
+-----------------+---------+----------------------+----------------------------------------------+-----------+-----------+----------+

So this way, your host machine and containers are in the same network.  It took me time to wait after lxc list is executed for the IP to appear. 


Also, as I have more than one container, I can do assignment in two ways: 
1. To assign manually to each running lxc config device add $name_of_container  eth0 nic nictype=bridged parent=bridge0 name=eth0
2. Apply the bridge to all existing containers using this little script: 
for container in $(lxc list --format csv | cut -d, -f1); do
    lxc config device add "$container" eth0 nic nictype=bridged parent=bridge0 name=eth0
done


Once done,update the default profile to ensure all newly created containers automatically use bridge0: lxc profile device add default eth0 nic nictype=bridged parent=bridge0 name=eth0

